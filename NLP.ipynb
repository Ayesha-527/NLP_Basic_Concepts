{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1J-Q3ChO2zfP",
        "outputId": "d3419ba4-3498-4fd1-89c2-24ff5090bf7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['She', 'is', 'a', 'good', 'dancer', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenization\n",
        "text = \"She is a good dancer.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Sentence Segmentation\n",
        "paragraph = \"She is a good dancer. He plays the guitar well. They enjoy singing together.\"\n",
        "sentences = sent_tokenize(paragraph)\n",
        "print(\"Sentences:\", sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2kJ4MX627nv",
        "outputId": "aaa1fa72-e71a-440d-87c6-90d6094e9a61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['She is a good dancer.', 'He plays the guitar well.', 'They enjoy singing together.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Az3cgo128_n",
        "outputId": "88fbec6f-edf8-4780-b7fa-e9b323d29c35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rocks : rock\n",
            "corpora : corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Updated regular expressions:\n",
        "\n",
        "# Regular expression for all alphabetic strings (letters only)\n",
        "regex_a = r'\\b[a-zA-Z]+\\b'\n",
        "\n",
        "# Regular expression for lowercase alphabetic strings ending in a 'b'\n",
        "regex_b = r'\\b[a-z]+b\\b'\n",
        "\n",
        "# Regular expression for strings with three consecutive repeated words\n",
        "regex_c = r'\\b(\\w+) \\1 \\1\\b'\n",
        "\n",
        "# Regular expression for strings with each 'a' immediately preceded and followed by a 'b'\n",
        "regex_d = r'\\b\\w?ab+\\w?\\b'\n",
        "\n",
        "# Regular expression for strings starting with an integer and ending with a word\n",
        "regex_e = r'^\\d+\\s.*\\b\\w+$'\n",
        "\n",
        "# Regular expression for strings containing both 'grotto' and 'raven'\n",
        "regex_f = r'\\bgrotto\\b.*\\braven\\b|\\braven\\b.*\\bgrotto\\b'\n",
        "\n",
        "# Regular expression for the first word of an English sentence\n",
        "regex_g = r'^[A-Z][^.!?]*'\n",
        "# Example text (from the provided draft)\n",
        "text = \"\"\"\n",
        "Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing..\n",
        "\"\"\"\n",
        "\n",
        "# Function to find matches using a given regex pattern\n",
        "def find_matches(pattern, text):\n",
        "    return re.findall(pattern, text, flags=re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    # Test the regular expressions on the example text\n",
        "matches_a = find_matches(regex_a, text)\n",
        "matches_b = find_matches(regex_b, text)\n",
        "matches_c = find_matches(regex_c, text)\n",
        "matches_d = find_matches(regex_d, text)\n",
        "matches_e = find_matches(regex_e, text)\n",
        "matches_f = find_matches(regex_f, text)\n",
        "matches_g = find_matches(regex_g, text)\n",
        "\n",
        "# Print the results\n",
        "print(\"Matches for regex_a:\")\n",
        "print(matches_a)\n",
        "\n",
        "print(\"\\nMatches for regex_b:\")\n",
        "print(matches_b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLxR9_zp3BcL",
        "outputId": "eec1403b-6733-4c21-f115-3ac4ba4eecaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matches for regex_a:\n",
            "['Up', 'to', 'the', 'most', 'natural', 'language', 'processing', 'systems', 'were', 'based', 'on', 'complex', 'sets', 'of', 'hand', 'written', 'rules', 'Starting', 'in', 'the', 'late', 'however', 'there', 'was', 'a', 'revolution', 'in', 'natural', 'language', 'processing', 'with', 'the', 'introduction', 'of', 'machine', 'learning', 'algorithms', 'for', 'language', 'processing']\n",
            "\n",
            "Matches for regex_b:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers python-Levenshtein\n",
        "from Levenshtein import distance\n",
        "\n",
        "# Levenshtein Distance\n",
        "def levenshtein_distance(str1, str2):\n",
        "    # Create a matrix to store distances\n",
        "    matrix = [[0] * (len(str2) + 1) for _ in range(len(str1) + 1)]\n",
        "\n",
        "    # Initialize the matrix\n",
        "    for i in range(len(str1) + 1):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(len(str2) + 1):\n",
        "        matrix[0][j] = j\n",
        "\n",
        "    # Fill in the matrix using dynamic programming\n",
        "    for i in range(1, len(str1) + 1):\n",
        "        for j in range(1, len(str2) + 1):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 1\n",
        "            matrix[i][j] = min(\n",
        "                matrix[i - 1][j] + 1,\n",
        "                matrix[i][j - 1] + 1,\n",
        "                matrix[i - 1][j - 1] + cost\n",
        "            )\n",
        "\n",
        "    # Return the Levenshtein distance\n",
        "    return matrix[len(str1)][len(str2)]\n",
        "\n",
        "# Example Levenshtein Distance\n",
        "str1 = \"NaturalLanguageProcessing\"\n",
        "str2 = \"NaturalLanguageUnderstanding\"\n",
        "print(f\"Levenshtein Distance between '{str1}' and '{str2}': {levenshtein_distance(str1, str2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEFhBSZI3eBO",
        "outputId": "dee98389-0db3-4539-82bf-0e96f76ce817"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.23.0-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.19.4)\n",
            "Collecting Levenshtein==0.23.0 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.23.0->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.11.17)\n",
            "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 python-Levenshtein-0.23.0 rapidfuzz-3.5.2\n",
            "Levenshtein Distance between 'NaturalLanguageProcessing' and 'NaturalLanguageUnderstanding': 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def get_stats(vocab):\n",
        "\t\"\"\"\n",
        "\tGiven a vocabulary (dictionary mapping words to frequency counts), returns a\n",
        "\tdictionary of tuples representing the frequency count of pairs of characters\n",
        "\tin the vocabulary.\n",
        "\t\"\"\"\n",
        "\tpairs = defaultdict(int)\n",
        "\tfor word, freq in vocab.items():\n",
        "\t\tsymbols = word.split()\n",
        "\t\tfor i in range(len(symbols)-1):\n",
        "\t\t\tpairs[symbols[i],symbols[i+1]] += freq\n",
        "\treturn pairs\n",
        "\n",
        "def merge_vocab(pair, v_in):\n",
        "\t\"\"\"\n",
        "\tGiven a pair of characters and a vocabulary, returns a new vocabulary with the\n",
        "\tpair of characters merged together wherever they appear.\n",
        "\t\"\"\"\n",
        "\tv_out = {}\n",
        "\tbigram = re.escape(' '.join(pair))\n",
        "\tp = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "\tfor word in v_in:\n",
        "\t\tw_out = p.sub(''.join(pair), word)\n",
        "\t\tv_out[w_out] = v_in[word]\n",
        "\treturn v_out\n",
        "\n",
        "def get_vocab(data):\n",
        "\t\"\"\"\n",
        "\tGiven a list of strings, returns a dictionary of words mapping to their frequency\n",
        "\tcount in the data.\n",
        "\t\"\"\"\n",
        "\tvocab = defaultdict(int)\n",
        "\tfor line in data:\n",
        "\t\tfor word in line.split():\n",
        "\t\t\tvocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "\treturn vocab\n",
        "\n",
        "def byte_pair_encoding(data, n):\n",
        "\t\"\"\"\n",
        "\tGiven a list of strings and an integer n, returns a list of n merged pairs\n",
        "\tof characters found in the vocabulary of the input data.\n",
        "\t\"\"\"\n",
        "\tvocab = get_vocab(data)\n",
        "\tfor i in range(n):\n",
        "\t\tpairs = get_stats(vocab)\n",
        "\t\tbest = max(pairs, key=pairs.get)\n",
        "\t\tvocab = merge_vocab(best, vocab)\n",
        "\treturn vocab\n",
        "\n",
        "# Example usage:\n",
        "corpus = '''Tokenization is the process of breaking down\n",
        "a sequence of text into smaller units called tokens,\n",
        "which can be words, phrases, or even individual characters.\n",
        "Tokenization is often the first step in natural languages processing tasks\n",
        "such as text classification, named entity recognition, and sentiment analysis.\n",
        "The resulting tokens are typically used as input to further processing steps,\n",
        "such as vectorization, where the tokens are converted\n",
        "into numerical representations for machine learning models to use.'''\n",
        "data = corpus.split('.')\n",
        "\n",
        "n = 230\n",
        "bpe_pairs = byte_pair_encoding(data, n)\n",
        "bpe_pairs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JESa-VoJ3-Vr",
        "outputId": "2637feca-e079-4cd1-9ed4-12d0d05f6527"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Tokenization</w>': 2,\n",
              " 'is</w>': 2,\n",
              " 'the</w>': 3,\n",
              " 'process</w>': 1,\n",
              " 'of</w>': 2,\n",
              " 'breaking</w>': 1,\n",
              " 'down</w>': 1,\n",
              " 'a</w>': 1,\n",
              " 'sequence</w>': 1,\n",
              " 'text</w>': 2,\n",
              " 'into</w>': 2,\n",
              " 'smaller</w>': 1,\n",
              " 'units</w>': 1,\n",
              " 'called</w>': 1,\n",
              " 'tokens,</w>': 1,\n",
              " 'which</w>': 1,\n",
              " 'can</w>': 1,\n",
              " 'be</w>': 1,\n",
              " 'words,</w>': 1,\n",
              " 'phrases,</w>': 1,\n",
              " 'or</w>': 1,\n",
              " 'even</w>': 1,\n",
              " 'individual</w>': 1,\n",
              " 'characters</w>': 1,\n",
              " 'often</w>': 1,\n",
              " 'first</w>': 1,\n",
              " 'step</w>': 1,\n",
              " 'in</w>': 1,\n",
              " 'natural</w>': 1,\n",
              " 'languages</w>': 1,\n",
              " 'processing</w>': 2,\n",
              " 'tasks</w>': 1,\n",
              " 'such</w>': 2,\n",
              " 'as</w>': 3,\n",
              " 'classification,</w>': 1,\n",
              " 'named</w>': 1,\n",
              " 'entity</w>': 1,\n",
              " 'recognition,</w>': 1,\n",
              " 'and</w>': 1,\n",
              " 'sentiment</w>': 1,\n",
              " 'analysis</w>': 1,\n",
              " 'The</w>': 1,\n",
              " 'resulting</w>': 1,\n",
              " 'tokens</w>': 2,\n",
              " 'are</w>': 2,\n",
              " 'typically</w>': 1,\n",
              " 'used</w>': 1,\n",
              " 'input</w>': 1,\n",
              " 'to</w>': 2,\n",
              " 'further</w>': 1,\n",
              " 'steps,</w>': 1,\n",
              " 'vectorization,</w>': 1,\n",
              " 'where</w>': 1,\n",
              " 'converted</w>': 1,\n",
              " 'numerical</w>': 1,\n",
              " 'representations</w>': 1,\n",
              " 'for</w>': 1,\n",
              " 'machine</w>': 1,\n",
              " 'learning</w>': 1,\n",
              " 'models</w>': 1,\n",
              " 'use</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "# Sample text\n",
        "text = \"Natural language processing (NLP) is a branch of artificial intelligence (AI) that enables computers to comprehend, generate, and manipulate human language.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Part of Speech Tagging\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"Part of Speech Tags:\", pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjINvxxN5VJs",
        "outputId": "f8524d80-d717-431e-9146-1070903689d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Part of Speech Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('branch', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'IN'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('comprehend', 'VB'), (',', ','), ('generate', 'NN'), (',', ','), ('and', 'CC'), ('manipulate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Sample text for the language model\n",
        "text = \"This is a sample text for building n-gram language models. We'll use NLTK for this demonstration.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text.lower())  # Convert text to lowercase for simplicity\n",
        "token_count = len(tokens)\n",
        "\n",
        "# Create unigrams, bigrams, and trigrams\n",
        "unigrams = ngrams(tokens, 1)\n",
        "bigrams = ngrams(tokens, 2)\n",
        "trigrams = ngrams(tokens, 3)\n",
        "\n",
        "# Frequency distribution of n-grams\n",
        "unigram_freq = FreqDist(unigrams)\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "trigram_freq = FreqDist(trigrams)\n",
        "\n",
        "# Display results\n",
        "print(\"Unigrams:\")\n",
        "print(unigram_freq.most_common())\n",
        "\n",
        "print(\"\\nBigrams:\")\n",
        "print(bigram_freq.most_common())\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "print(trigram_freq.most_common())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq-kKWGN-5Xe",
        "outputId": "886a6012-c7bd-45bc-a862-1966a0531706"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "[(('this',), 2), (('for',), 2), (('.',), 2), (('is',), 1), (('a',), 1), (('sample',), 1), (('text',), 1), (('building',), 1), (('n-gram',), 1), (('language',), 1), (('models',), 1), (('we',), 1), ((\"'ll\",), 1), (('use',), 1), (('nltk',), 1), (('demonstration',), 1)]\n",
            "\n",
            "Bigrams:\n",
            "[(('this', 'is'), 1), (('is', 'a'), 1), (('a', 'sample'), 1), (('sample', 'text'), 1), (('text', 'for'), 1), (('for', 'building'), 1), (('building', 'n-gram'), 1), (('n-gram', 'language'), 1), (('language', 'models'), 1), (('models', '.'), 1), (('.', 'we'), 1), (('we', \"'ll\"), 1), ((\"'ll\", 'use'), 1), (('use', 'nltk'), 1), (('nltk', 'for'), 1), (('for', 'this'), 1), (('this', 'demonstration'), 1), (('demonstration', '.'), 1)]\n",
            "\n",
            "Trigrams:\n",
            "[(('this', 'is', 'a'), 1), (('is', 'a', 'sample'), 1), (('a', 'sample', 'text'), 1), (('sample', 'text', 'for'), 1), (('text', 'for', 'building'), 1), (('for', 'building', 'n-gram'), 1), (('building', 'n-gram', 'language'), 1), (('n-gram', 'language', 'models'), 1), (('language', 'models', '.'), 1), (('models', '.', 'we'), 1), (('.', 'we', \"'ll\"), 1), (('we', \"'ll\", 'use'), 1), ((\"'ll\", 'use', 'nltk'), 1), (('use', 'nltk', 'for'), 1), (('nltk', 'for', 'this'), 1), (('for', 'this', 'demonstration'), 1), (('this', 'demonstration', '.'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Sample text for the language model\n",
        "text = \"This is a sample text for building n-gram language models. We'll use NLTK for this demonstration.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text.lower())  # Convert text to lowercase for simplicity\n",
        "\n",
        "# Create n-grams\n",
        "n = 3  # You can change 'n' to calculate perplexity for different n-gram models\n",
        "ngrams_list = list(ngrams(tokens, n))\n",
        "\n",
        "# Calculate frequencies of n-grams\n",
        "ngrams_freq = Counter(ngrams_list)\n",
        "\n",
        "# Total number of n-grams\n",
        "total_ngrams = len(ngrams_list)\n",
        "\n",
        "# Calculate perplexity\n",
        "perplexity = math.pow(2, -sum(math.log2(ngrams_freq[gram] / (total_ngrams - (n - 1))) for gram in ngrams_list) / total_ngrams)\n",
        "print(f\"Perplexity of {n}-gram model:\", perplexity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32_bLNltCp8K",
        "outputId": "48f3493b-8204-4bcd-afd1-ad136a821958"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity of 3-gram model: 14.999999999999993\n"
          ]
        }
      ]
    }
  ]
}